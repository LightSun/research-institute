

================== 深入理解机器学习 =========================
[ 线性回归 ]:
    数据的关系:
        y = mx + b
          m: 斜率，
          b：y轴截距
          x: 输入特征
          y: 试图预测的值

    线性模型方程式:
        y’= b + w1x1
          b: 偏差（y轴截距）
          w1: 指特征1的权重
          x1: 输入的特征（已知输入项）
          y': 预测标签（理想输出值）

    ps: 如果具有3个特征,则 y' = b + w1x1 + w2x2 + w3x3

[ 训练与损失 ]:
    训练模型：表示通过有标签的样本来学习(确定)所有权重和偏差的理想值.
    训练模型的目标: 从所有样本中找到一组平均损失‘较小’的权重和偏差 。
    经验风险最小化.(监督式学习)


    损失函数:
        平方损失：  线性回归模型使用的是平方损失（L2损失）
        均方误差：  每个样本的平均平方损失. (方差/样本数)



========================== 降低损失 ===========================
学习目标
     了解如何使用--迭代方法-来训练模型。

全面了解梯度下降法和一些变体，包括：
    小批量梯度下降法
    随机梯度下降法
    尝试不同的学习速率

梯度下降法:
     起点，计算损失曲线在顶点处的梯度 。
     梯度是偏导数的矢量,具有2个特征：方向和大小.
     梯度始终沿着损失函数最为迅猛的方向, 梯度下降法会沿着负梯度的方向走一步，以便尽快降低损失。

     depend on: 负梯度
     从一个点移动到另一个点，不断迭代.

学习速率:
     超参数
     梯度下降算法用梯度乘以一个称为学习速率(或叫步长)的标量, 用以确定下一个点的位置.
     eg: 梯度大小2.5， 学习速率0.1 则梯度下降算法会选择距离前一个点0.025的位置。

     学习速率为调整的重点，过小了会导致学习需要花太多时间，过大了会导致偏离我们试验的目标。
     eg： 跳过了损失函数的最低点。


优化学习速率

随机梯度下降法(SGD)：
     批量： 指用于在单次迭代中计算梯度的样本总数。
     小批量随机梯度下降法
